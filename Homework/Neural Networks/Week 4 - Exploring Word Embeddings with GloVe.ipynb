{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Week 4 - Exploring Word Embeddings with GloVe",
   "id": "15b5bf6b15b66820"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 1. Introduction & Objectives\n",
    "\n",
    "In this notebook, we’ll explore GloVe word embeddings and use them to perform some simple yet powerful word vector operations. Our main objectives are:\n",
    "\n",
    "- Learn how to use pre-trained GloVe embeddings for natural language processing tasks.\n",
    "- Get a better understanding of how word vectors capture relationships between words.\n",
    "\n",
    "We’ll start by loading the GloVe embeddings from a local file and extracting the word vectors for three words: **\"man\"**, **\"woman\"**, and **\"king\"**. Then, we’ll calculate the result of the operation:\n",
    "\n",
    "`vec(\"woman\") - vec(\"man\") + vec(\"king\")`\n",
    "\n",
    "Finally, we’ll find the word closest to this result using cosine similarity. This exercise demonstrates how pre-trained embeddings can represent word relationships in a meaningful way, making it easier to analyze and interpret language."
   ],
   "id": "b6c9dc2d5831c85"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 2. Data Understanding\n",
    "\n",
    "In this notebook, we’ll work with the **GloVe 6B 100d embeddings**, a set of pre-trained word vectors provided by the Stanford NLP Group. These embeddings are derived from a massive corpus of 6 billion tokens, including text from Wikipedia and Gigaword. Each word is represented as a dense vector with 100 dimensions, capturing its semantic meaning and relationships with other words.\n",
    "\n",
    "The GloVe 6B embeddings include a vocabulary of 400,000 words, making them versatile for a wide range of natural language processing tasks. For this assignment, we’ll specifically use the `glove.6B.100d.txt` file, which contains the 100-dimensional embeddings.\n"
   ],
   "id": "df07d2b2f3afb91"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### 2.1 Importing Libraries and Loading GloVe Embeddings\n",
    "\n",
    "We begin by importing the necessary libraries and loading the GloVe embeddings from the `glove.6B.100d.txt` file. The embeddings will be stored in a dictionary, with words as keys and their corresponding vectors as values."
   ],
   "id": "698d77a090a0d2db"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-11-19T01:31:59.401670Z",
     "start_time": "2024-11-19T01:31:59.300962Z"
    }
   },
   "source": [
    "# Importing Libraries\n",
    "import numpy as np"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Next, we will load the GloVe embeddings from the `glove.6B.100d.txt` file. The function `load_glove_embeddings` reads the file line by line, extracts the word and its vector, and stores them in a dictionary.",
   "id": "ea307d7419e15f5b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-19T01:32:15.632163Z",
     "start_time": "2024-11-19T01:31:59.406327Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Load the GloVe embeddings\n",
    "def load_glove_embeddings(path):\n",
    "    embeddings = {}\n",
    "\n",
    "    with open(path, 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            vector = np.array(values[1:], dtype='float32')\n",
    "            embeddings[word] = vector\n",
    "\n",
    "    return embeddings\n",
    "\n",
    "\n",
    "# File path for the GloVe embeddings\n",
    "file_path = '../../Inputs/glove.6B.100d.txt'\n",
    "\n",
    "# Load the GloVe embeddings\n",
    "glove_embeddings = load_glove_embeddings(file_path)"
   ],
   "id": "cc3dcaa51eda102c",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "The GloVe embeddings have been successfully loaded into the `glove_embeddings` dictionary. We can now proceed to extract the word vectors for the specified words.",
   "id": "cabddfc0c1ba5a26"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 3. Extracting Word Vectors\n",
    "\n",
    "Let's now extract the word vectors for the words **woman**, **man**, and **king**. We'll use these vectors to perform the necessary operations and analyze the relationships between the words."
   ],
   "id": "6fbf881a8f8a4295"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-19T01:32:15.961381Z",
     "start_time": "2024-11-19T01:32:15.958501Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Extract word vectors\n",
    "woman_vect = glove_embeddings['woman']\n",
    "man_vect = glove_embeddings['man']\n",
    "king_vect = glove_embeddings['king']"
   ],
   "id": "5375b5d7f751ac9e",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "The word vectors have been extracted. Next, we'll calculate the result of the operation `vec(\"man\") - vec(\"woman\") + vec(\"king\")`.",
   "id": "f100aebbf9d5b450"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-19T01:32:16.139148Z",
     "start_time": "2024-11-19T01:32:16.136331Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Calculate the result of the operation\n",
    "result_vect = woman_vect - man_vect + king_vect"
   ],
   "id": "76e631624f36ea0f",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We have successfully calculated the resulting vector. Next, we'll find the word closest to this result using cosine similarity.",
   "id": "3f056adb8d1f4484"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 4. Finding the Closest Word\n",
    "\n",
    "To find the word closest to the result vector, we'll calculate the cosine similarity between the result vector and each word vector in the GloVe embeddings. The word with the highest cosine similarity will be considered the closest to the result."
   ],
   "id": "77e98dd197ce9531"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-19T01:32:20.070579Z",
     "start_time": "2024-11-19T01:32:16.311844Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Calculate cosine similarity between two vectors\n",
    "def cosine_similarity(vec1, vec2):\n",
    "    dot_product = np.dot(vec1, vec2)\n",
    "    norm_vec1 = np.linalg.norm(vec1)\n",
    "    norm_vec2 = np.linalg.norm(vec2)\n",
    "\n",
    "    # Avoid division by zero\n",
    "    if norm_vec1 == 0 or norm_vec2 == 0:\n",
    "        return 0\n",
    "\n",
    "    return dot_product / (norm_vec1 * norm_vec2)\n",
    "\n",
    "\n",
    "# Find the closest word\n",
    "def find_closest_word(result_vec, embeddings):\n",
    "    closest_word = None\n",
    "    max_similarity = -1\n",
    "\n",
    "    for word, vector in embeddings.items():\n",
    "        similarity = cosine_similarity(result_vec, vector)\n",
    "\n",
    "        if similarity > max_similarity:\n",
    "            closest_word = word\n",
    "            max_similarity = similarity\n",
    "\n",
    "    return closest_word\n",
    "\n",
    "\n",
    "# Assuming `result_vect` and `glove_embeddings` are defined\n",
    "closest_word = find_closest_word(result_vect, glove_embeddings)"
   ],
   "id": "9ae73f72b503e5d2",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "The word closest to the result vector has been successfully identified. We can move onto checking the results.",
   "id": "8278997e9c43c3b7"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### 4.1 Finding the Closest 5 Words\n",
    "\n",
    "Let's find the 5 words closest to the result vector using cosine similarity. This will give us a better understanding of the relationships captured by the word vectors."
   ],
   "id": "4eff07050c41db35"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-19T01:32:24.145731Z",
     "start_time": "2024-11-19T01:32:20.248168Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Find the 5 closest words\n",
    "def find_closest_words(result_vec, embeddings, n=5):\n",
    "    closest_words = []\n",
    "\n",
    "    for word, vector in embeddings.items():\n",
    "        similarity = cosine_similarity(result_vec, vector)\n",
    "        closest_words.append((word, similarity))\n",
    "\n",
    "    # Sort the words by similarity\n",
    "    closest_words.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    return closest_words[:n]\n",
    "\n",
    "\n",
    "# Find the 5 closest words to the result vector\n",
    "closest_words = find_closest_words(result_vect, glove_embeddings, n=5)"
   ],
   "id": "855fed8e76d208fb",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 5. Findings\n",
    "\n",
    "Let's print the result of the operation and the word closest to the result vector."
   ],
   "id": "2de4b188fd1d5da0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-19T01:32:24.331585Z",
     "start_time": "2024-11-19T01:32:24.327732Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Print the result and the closest word\n",
    "print(f\"Result of the operation: {result_vect}\")"
   ],
   "id": "1595483b1f141d83",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result of the operation: [-0.10231996 -0.81294     0.10211001  0.985924    0.34218282  1.09095\n",
      " -0.48913    -0.05616698 -0.21029997 -1.02996    -0.86851     0.36786997\n",
      "  0.01960999  0.59259    -0.231901   -1.016919   -0.012184   -1.17194\n",
      " -0.52329     0.60645    -0.98537004 -1.001028    0.48913902  0.630072\n",
      "  0.58224     0.15908998  0.43684998 -1.25351     0.97054005 -0.06552899\n",
      "  0.733763    0.44219002  1.2091839   0.19698    -0.15948     0.34364\n",
      " -0.46222997  0.33772     0.14792703 -0.24959499 -0.77093005  0.522717\n",
      " -0.12830001 -0.91881    -0.01755    -0.44041002 -0.52656496  0.33734798\n",
      "  0.60639    -0.45067    -0.04158002  0.08408298  1.31456     0.67737997\n",
      " -0.24316001 -2.071      -0.60648996  0.19710997  0.63567     0.07819999\n",
      "  0.49161002  0.08172001  0.708557    0.201938    0.5155501  -0.23025298\n",
      " -0.40473     0.39212003 -0.5093     -0.139153    0.21609999 -0.628671\n",
      "  0.08894001  0.49167    -0.06637001  0.76095    -0.19442001  0.41131\n",
      " -1.04476    -0.14801991 -0.098355   -0.25115     0.808957    0.363129\n",
      " -0.78200996 -0.10483998  0.08340102 -1.24067     0.655344   -0.93635\n",
      "  0.648379   -0.55827     0.45621303  0.27575803 -1.54896    -0.19909596\n",
      " -0.50804996 -0.13818002  0.27731198 -0.75721   ]\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Here we can see the resulting vector obtained by performing the operation.\n",
    "\n",
    "Next, let's print the word closest to this result vector."
   ],
   "id": "3acebf950e1c47b6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-19T01:32:24.516555Z",
     "start_time": "2024-11-19T01:32:24.513160Z"
    }
   },
   "cell_type": "code",
   "source": "print(f\"Closest word to the result vector: {closest_word}\")",
   "id": "c402d6c85833b64b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closest word to the result vector: king\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "As observed, the word closest to the result vector is **king**. This highlights the ability of word embeddings to capture semantic relationships between words. Through simple vector operations, we can uncover meaningful insights and identify words that are closely related or similar in meaning.\n",
    "\n",
    "In this case, the result vector aligns most closely with **king** due to how the GloVe embeddings represent semantic relationships in the vector space. While the operation aimed to approximate the vector for **queen** (`woman - man + king`), the closest match is **king** because:\n",
    "- The embeddings may have clustered related words like \"king\" and \"queen\" in close proximity, with \"king\" being slightly closer to the resulting vector.\n",
    "- Cosine similarity measures direction rather than exact magnitude, and the resulting vector's direction may align more closely with \"king.\"\n",
    "- GloVe embeddings, trained on general corpora, may not perfectly distinguish nuanced relationships such as gender-specific roles.\n",
    "\n",
    "Finally, let's print the 5 words closest to the result vector using cosine similarity to further analyze the embedding space."
   ],
   "id": "baf37907ae3bec26"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-19T01:32:24.693432Z",
     "start_time": "2024-11-19T01:32:24.689645Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Print the 5 closest words\n",
    "print(\"5 closest words to the result vector:\")\n",
    "for word, similarity in closest_words:\n",
    "    print(f\"{word}: {similarity:.4f}\")"
   ],
   "id": "3e754bc511549d1d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 closest words to the result vector:\n",
      "king: 0.8552\n",
      "queen: 0.7834\n",
      "monarch: 0.6934\n",
      "throne: 0.6833\n",
      "daughter: 0.6809\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "The 5 words closest to the result vector have been successfully identified:\n",
    "\n",
    "- **king**: 0.8552\n",
    "- **queen**: 0.7834\n",
    "- **monarch**: 0.6934\n",
    "- **throne**: 0.6833\n",
    "- **daughter**: 0.6809\n",
    "\n",
    "This analysis offers valuable insights into the relationships between words and demonstrates how word embeddings effectively capture semantic connections.\n",
    "\n",
    "#### Why These Words Are Closest\n",
    "1. **King**:\n",
    "   - The closest match, **king**, aligns with the resulting vector because the operation starts with \"king\" and applies the gender transformation captured by `woman - man`. The GloVe embeddings cluster related concepts (royalty and gendered roles), causing \"king\" to remain very close.\n",
    "\n",
    "2. **Queen**:\n",
    "   - As the ideal result, \"queen\" is the second-closest word. It reflects the success of the analogy transformation, demonstrating how GloVe embeddings encode the relationship between male and female roles within the same domain (royalty).\n",
    "\n",
    "3. **Monarch**:\n",
    "   - This word is semantically related to both \"king\" and \"queen,\" as it represents a gender-neutral term for royalty. Its proximity highlights the embeddings' ability to generalize hierarchical relationships.\n",
    "\n",
    "4. **Throne**:\n",
    "   - The word \"throne\" is closely associated with royalty and leadership. Its presence among the closest words illustrates how embeddings capture contextual relationships (e.g., objects and concepts related to royalty).\n",
    "\n",
    "5. **Daughter**:\n",
    "   - Although slightly less related to royalty directly, \"daughter\" appears close due to the gender transformation in the operation. It reflects how GloVe embeddings cluster words based on gender-related themes.\n",
    "\n",
    "Overall, these results showcase the power of word embeddings in capturing both direct semantic relationships (e.g., king and queen) and broader contextual associations (e.g., monarch and throne)."
   ],
   "id": "a78a12765a40dace"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 6. Conclusions\n",
    "\n",
    "In this analysis, we explored the power of word embeddings, specifically GloVe, to capture semantic relationships and analogies between words. By performing simple vector arithmetic on word embeddings, we demonstrated how meaningful patterns and relationships can emerge.\n",
    "\n",
    "#### Key Findings:\n",
    "1. **Closest Word Analysis**:\n",
    "   - The closest word to the resulting vector from the operation `woman - man + king` was **king**, with **queen** as the second closest word. This highlights the strength of word embeddings in representing relationships, though subtle limitations exist in perfectly capturing gender-specific analogies.\n",
    "   - Other closely related words, such as **monarch** and **throne**, show how embeddings cluster related concepts, such as royalty and leadership, within the same semantic space.\n",
    "\n",
    "2. **Semantic Representation**:\n",
    "   - Word embeddings like GloVe effectively encode hierarchical, gendered, and contextual relationships between words. For example, the relationships between \"man,\" \"woman,\" \"king,\" and \"queen\" reflect both gender and role-specific transformations.\n",
    "\n",
    "3. **Utility of Word Embeddings**:\n",
    "   - This exercise demonstrates how word embeddings can be used to uncover insights into language semantics, solve analogies, and build downstream natural language processing models.\n",
    "\n",
    "#### Reflections:\n",
    "While the embeddings successfully identified \"king\" as the closest word, the slightly lower ranking of \"queen\" suggests room for improvement in embedding models' ability to distinguish nuanced relationships. Using higher-dimensional embeddings or alternative models like FastText or contextual embeddings (e.g., BERT) might yield even better results.\n",
    "\n",
    "#### Future Directions:\n",
    "This exploration lays the foundation for leveraging word embeddings in more advanced tasks, such as semantic similarity, text classification, or contextual understanding in natural language processing pipelines. Continued development of embedding models will further enhance their ability to capture language nuances.\n",
    "\n",
    "In summary, this analysis demonstrates the potential of word embeddings to transform linguistic data into structured, interpretable representations that are invaluable in understanding and processing natural language."
   ],
   "id": "1e255d54b818275d"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
