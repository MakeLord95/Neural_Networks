{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Week 1: Iris Dataset Classification with Keras",
   "id": "df271f5d51108e38"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 1. Introduction & Objectives\n",
    "\n",
    "In this notebook, we will use the Iris dataset to classify the species of iris flowers. \n",
    "\n",
    "Our objective for this notebook is to build a multiclass classifier with Keras, that takes the four numerical features of the iris samples as inputs and outputs the prediction for the species of the iris flowers. The classifier will include the input layer, one hidden layer, and the output layer. We will use the softmax activation function in the output layer and the categorical crossentropy loss function to train the model.\n"
   ],
   "id": "6fd3c9db8290af22"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "In this project, we will use the Iris dataset, which is available as a raw data file from the [UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data). This dataset consists of 150 samples of iris flowers, each with four numerical features: **sepal length**, **sepal width**, **petal length**, and **petal width**. The target label specifies the **species** of each iris flower.\n",
    "\n",
    "For model training and evaluation, we will divide the dataset as follows:\n",
    "\n",
    "- **Training set**: 80% of the total samples (120 samples)\n",
    "- **Validation set**: 10% of the training samples (12 samples from the training data)\n",
    "- **Test set**: 20% of the total samples (30 samples)\n",
    "\n",
    "To ensure an even distribution across the training, validation, and test sets, we will shuffle the data randomly before splitting. This setup provides sufficient data for training and allows us to evaluate the model's performance on separate validation and test sets.\n"
   ],
   "id": "86cc0375fa2447f8"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### 2.1 Importing Required Libraries and Loading the Dataset\n",
    "\n",
    "To start, we’ll disable TensorFlow warnings to keep the output clean, and explicitly set the Keras backend to TensorFlow for consistency in model development. \n",
    "\n",
    "Next, we’ll import the necessary libraries, including `pandas` for data manipulation, `numpy` for numerical operations, and `matplotlib.pyplot` for potential data visualization. We’ll also import essential Keras components such as `layers`, `Input`, `Model`, and `callbacks` to facilitate building and training the neural network.\n",
    "\n",
    "To load the Iris dataset, we’ll read directly from the UCI Machine Learning Repository’s raw data link. Using `pandas`, we’ll load the dataset into a DataFrame and assign column names to clarify the features: **sepal length**, **sepal width**, **petal length**, **petal width**, and the **species** label.\n"
   ],
   "id": "f70b624096d25d4c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Disabling the tensoflow warnings and setting the keras backend to tensorflow\n",
    "import os\n",
    "\n",
    "os.environ['KERAS_BACKEND'] = 'tensorflow'\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'"
   ],
   "id": "95dc0306fb9710cc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Importing the required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras import Input, Model\n",
    "from keras.src.layers import Dense\n",
    "from keras.src.callbacks import ModelCheckpoint"
   ],
   "id": "a4c390843d63ffe3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Loading the dataset\n",
    "url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data'\n",
    "column_names = ['sepal_length', 'sepal_width', 'petal_length', 'petal_width', 'species']\n",
    "dataset = pd.read_csv(url, names=column_names)"
   ],
   "id": "a52af18d0df71363",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "The dataset has been successfully loaded into a DataFrame. Let's take a look at the first few rows to understand the structure of the data.",
   "id": "fb79d961fa63e74a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Displaying the first few rows of the dataset\n",
    "dataset.head()"
   ],
   "id": "43c064466a60e2c8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "The dataset contains five columns: **sepal_length**, **sepal_width**, **petal_length**, **petal_width**, and **species**. The **species** column represents the target label for each sample, indicating the species of the iris flower. We can move onto the next step of preprocessing the data for training the model.",
   "id": "7ea9d9223c976492"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### 2.2 Preprocessing the Data\n",
    "\n",
    "Before training the model, we need to preprocess the data to ensure that it is suitable for training. In this step, we will perform the following preprocessing steps:\n",
    "\n",
    "    Randomly shuffle the data to ensure an even distribution across the training, validation, and test sets.\n",
    "    Split the features and target labels into separate variables.\n",
    "    One-hot encode the target labels to convert them into a binary matrix representation.\n",
    "    Split the data into training, validation, and test sets.\n",
    "    \n",
    "To shuffle the data, we will use `random.shuffle()` from the `numpy` library. This function will shuffle the indices of the samples in the dataset, allowing us to split the data randomly."
   ],
   "id": "12b43f4abf8a97af"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Shuffling the data\n",
    "np.random.seed(0)\n",
    "dataset = dataset.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "dataset.head()"
   ],
   "id": "faac30de1e3cb766",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "The data has been successfully shuffled. Next, we will split the features and target labels into separate variables and one-hot encode the target labels.",
   "id": "db78207d2d698c06"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Split the features and target labels\n",
    "features = dataset[['sepal_length', 'sepal_width', 'petal_length', 'petal_width']]\n",
    "target = dataset['species']\n",
    "\n",
    "# One-hot encode the target labels\n",
    "target = pd.get_dummies(target).astype(int)"
   ],
   "id": "a2a279224a9a3c19",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "The features have been separated from the target labels, and the target labels have been one-hot encoded. We can now split the data into training and testing sets.",
   "id": "8c24fa7a7cdfc656"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Define the split ratio (80% training, 20% testing)\n",
    "split = 0.8\n",
    "\n",
    "# Split the feature labels into training and testing sets\n",
    "split_index = int(split * len(dataset))\n",
    "train_data, test_data = features[:split_index], features[split_index:]\n",
    "\n",
    "# Split the target labels into training and testing sets\n",
    "train_target, test_target = target[:split_index], target[split_index:]"
   ],
   "id": "5b90f23e34cf36aa",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "The data has now been successfully split into training and testing sets. We can check the shape of the training and testing sets to ensure that the data has been split correctly.",
   "id": "8c04ce48426e0e13"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Display the shape of the training and testing sets\n",
    "train_data.shape, train_target.shape, test_data.shape, test_target.shape"
   ],
   "id": "840704ffaf0331d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "The training and testing sets contain 120 and 30 samples, respectively. We can now proceed to creating the model and fitting it to the training data.",
   "id": "555254bbada63a4f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 3. Creating the Model\n",
    "\n",
    "To build the neural network model, we will use the Keras functional API. The model will consist of the following layers:\n",
    "    \n",
    "        Input layer: Accepts the four numerical features of the iris samples.\n",
    "        Hidden layer: Contains 8 neurons and uses the ReLU activation function.\n",
    "        Output layer: Contains 3 neurons (one for each species) and uses the softmax activation function."
   ],
   "id": "16b33787b5680ee7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Define the input layer\n",
    "inputs = Input(shape=(4,), name='input')\n",
    "\n",
    "# Define the hidden layer\n",
    "hidden = Dense(8, activation='relu', name='hidden')(inputs)\n",
    "\n",
    "# Define the output layer\n",
    "outputs = Dense(3, activation='softmax', name='output')(hidden)\n",
    "\n",
    "# Create the model\n",
    "model = Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "# Display the model summary\n",
    "model.summary()"
   ],
   "id": "51df19deae55354c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "The model has been successfully created with the input, hidden, and output layers. The model summary provides information about the layers, including the number of parameters in each layer. We can now proceed to compiling and fitting the model to the training data.",
   "id": "7982dab91cb4a7f2"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 4. Compiling and Fitting the Model\n",
    "\n",
    "Before training the model, we need to compile it by specifying the optimizer, loss function, and evaluation metric. In this case, we will use the Adam optimizer, categorical crossentropy loss function, and accuracy as the evaluation metric. \n",
    "\n",
    "We will implement a callback to save the best model based on the validation loss during training. This callback will help us avoid overfitting by saving the model with the lowest validation loss.\n",
    "\n",
    "During the training, we will also save the best model based on the validation loss using the `ModelCheckpoint` callback."
   ],
   "id": "7f471c3278eb95f9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Define the ModelCheckpoint callback\n",
    "callbacks = [ModelCheckpoint(filepath='../Models/iris_model.keras', monitor='val_loss', save_best_only=True)]\n",
    "\n",
    "# Fit the model to the training data\n",
    "history = model.fit(train_data, train_target, epochs=100, batch_size=16, validation_split=0.1, callbacks=callbacks)"
   ],
   "id": "b7fc5e38a4864c64",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "b7c510baf7f78d47",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
